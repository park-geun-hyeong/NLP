{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA(Latent Dirichlet Allocation)\n",
    "- 문서의 집합에서 토픽을 찾아내는 프로세스 즉 토픽 모델링의 대표적 알고리즘\n",
    "- LDA DTM 혹은 TF-IDF행렬을 입력으로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=42, remove=('headers','footers',' quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = dataset.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "def cleaning(text):\n",
    "    text = re.sub('[^A-Za-z]',' ',text)\n",
    "    text = text.lower()\n",
    "    text = [i for i in text.split() if len(i)>2]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'document': document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc = [cleaning(i) for i in df['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_doc'] = clean_doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>[was, wondering, anyone, out, there, could, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>[fair, number, brave, souls, who, upgraded, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>[well, folks, mac, plus, finally, gave, the, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n&gt; a...</td>\n",
       "      <td>[robert, kyanko, rob, rjck, uucp, wrote, abrax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>[from, article, owcb, world, std, com, tombake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>[from, nyeda, cnsvax, uwec, edu, david, nye, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>I have a (very old) Mac 512k and a Mac Plus, b...</td>\n",
       "      <td>[have, very, old, mac, and, mac, plus, both, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>[just, installed, cpu, clone, motherboard, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>In article &lt;1qkgbuINNs9n@shelley.u.washington....</td>\n",
       "      <td>[article, qkgbuinns, shelley, washington, edu,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>Stolen from Pasadena between 4:30 and 6:30 pm ...</td>\n",
       "      <td>[stolen, from, pasadena, between, and, blue, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      I was wondering if anyone out there could enli...   \n",
       "1      A fair number of brave souls who upgraded thei...   \n",
       "2      well folks, my mac plus finally gave up the gh...   \n",
       "3      Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> a...   \n",
       "4      From article <C5owCB.n3p@world.std.com>, by to...   \n",
       "...                                                  ...   \n",
       "11309  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...   \n",
       "11310  I have a (very old) Mac 512k and a Mac Plus, b...   \n",
       "11311  I just installed a DX2-66 CPU in a clone mothe...   \n",
       "11312  In article <1qkgbuINNs9n@shelley.u.washington....   \n",
       "11313  Stolen from Pasadena between 4:30 and 6:30 pm ...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      [was, wondering, anyone, out, there, could, en...  \n",
       "1      [fair, number, brave, souls, who, upgraded, th...  \n",
       "2      [well, folks, mac, plus, finally, gave, the, g...  \n",
       "3      [robert, kyanko, rob, rjck, uucp, wrote, abrax...  \n",
       "4      [from, article, owcb, world, std, com, tombake...  \n",
       "...                                                  ...  \n",
       "11309  [from, nyeda, cnsvax, uwec, edu, david, nye, n...  \n",
       "11310  [have, very, old, mac, and, mac, plus, both, w...  \n",
       "11311  [just, installed, cpu, clone, motherboard, and...  \n",
       "11312  [article, qkgbuinns, shelley, washington, edu,...  \n",
       "11313  [stolen, from, pasadena, between, and, blue, a...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [was, wondering, anyone, out, there, could, en...\n",
       "1    [fair, number, brave, souls, who, upgraded, th...\n",
       "2    [well, folks, mac, plus, finally, gave, the, g...\n",
       "3    [robert, kyanko, rob, rjck, uucp, wrote, abrax...\n",
       "4    [from, article, owcb, world, std, com, tombake...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df.clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79250"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary) ## 단어의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in df.clean_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 4), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 6), (43, 1), (44, 4), (45, 4), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[0]) ## 44,45와 mapping된 단어는 4번이 사용되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics= 20, id2word=dictionary, passes=15)\n",
    "## topic을 20개로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 앞의 수치는 각 토픽에대한 단어의 기여도를 말한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.042*\"the\" + 0.023*\"with\" + 0.022*\"and\" + 0.020*\"drive\" + 0.015*\"card\" + 0.014*\"have\" + 0.014*\"for\" + 0.012*\"scsi\" + 0.010*\"system\" + 0.010*\"this\"')\n",
      "============================================================\n",
      "(1, '0.149*\"edu\" + 0.120*\"writes\" + 0.111*\"article\" + 0.099*\"com\" + 0.056*\"apr\" + 0.012*\"you\" + 0.010*\"news\" + 0.010*\"uiuc\" + 0.010*\"netcom\" + 0.008*\"org\"')\n",
      "============================================================\n",
      "(2, '0.042*\"scx\" + 0.017*\"chz\" + 0.016*\"gcx\" + 0.013*\"sandvik\" + 0.012*\"rlk\" + 0.012*\"rck\" + 0.010*\"kent\" + 0.009*\"uww\" + 0.009*\"syx\" + 0.009*\"mcx\"')\n",
      "============================================================\n",
      "(3, '0.074*\"the\" + 0.020*\"and\" + 0.014*\"team\" + 0.013*\"game\" + 0.012*\"for\" + 0.010*\"will\" + 0.010*\"was\" + 0.009*\"year\" + 0.008*\"but\" + 0.008*\"games\"')\n",
      "============================================================\n",
      "(4, '0.066*\"the\" + 0.035*\"that\" + 0.026*\"and\" + 0.021*\"you\" + 0.018*\"not\" + 0.014*\"are\" + 0.013*\"this\" + 0.011*\"have\" + 0.010*\"for\" + 0.009*\"but\"')\n",
      "============================================================\n",
      "(5, '0.017*\"pat\" + 0.016*\"digex\" + 0.016*\"helmet\" + 0.014*\"lib\" + 0.012*\"henrik\" + 0.009*\"libxmu\" + 0.009*\"xmu\" + 0.009*\"cyprus\" + 0.008*\"access\" + 0.008*\"com\"')\n",
      "============================================================\n",
      "(6, '0.026*\"pitt\" + 0.022*\"gordon\" + 0.020*\"banks\" + 0.016*\"geb\" + 0.013*\"surrender\" + 0.013*\"skepticism\" + 0.012*\"shameful\" + 0.012*\"intellect\" + 0.012*\"cadre\" + 0.012*\"soon\"')\n",
      "============================================================\n",
      "(7, '0.030*\"van\" + 0.022*\"win\" + 0.022*\"chi\" + 0.022*\"cal\" + 0.021*\"det\" + 0.020*\"bos\" + 0.018*\"tor\" + 0.016*\"mon\" + 0.016*\"pit\" + 0.015*\"que\"')\n",
      "============================================================\n",
      "(8, '0.048*\"the\" + 0.022*\"entry\" + 0.014*\"output\" + 0.014*\"file\" + 0.010*\"not\" + 0.009*\"entries\" + 0.009*\"program\" + 0.008*\"build\" + 0.007*\"int\" + 0.007*\"section\"')\n",
      "============================================================\n",
      "(9, '0.022*\"pts\" + 0.017*\"period\" + 0.011*\"air\" + 0.010*\"lemieux\" + 0.008*\"power\" + 0.008*\"jagr\" + 0.007*\"francis\" + 0.007*\"qvf\" + 0.006*\"play\" + 0.006*\"stevens\"')\n",
      "============================================================\n",
      "(10, '0.077*\"the\" + 0.035*\"and\" + 0.017*\"for\" + 0.011*\"will\" + 0.010*\"key\" + 0.009*\"this\" + 0.008*\"space\" + 0.007*\"are\" + 0.007*\"with\" + 0.006*\"from\"')\n",
      "============================================================\n",
      "(11, '0.053*\"the\" + 0.025*\"and\" + 0.020*\"for\" + 0.014*\"you\" + 0.012*\"this\" + 0.010*\"can\" + 0.009*\"are\" + 0.009*\"with\" + 0.008*\"have\" + 0.008*\"that\"')\n",
      "============================================================\n",
      "(12, '0.054*\"for\" + 0.020*\"price\" + 0.016*\"sale\" + 0.014*\"new\" + 0.011*\"offer\" + 0.011*\"shipping\" + 0.010*\"sell\" + 0.010*\"please\" + 0.009*\"condition\" + 0.008*\"interested\"')\n",
      "============================================================\n",
      "(13, '0.011*\"runs\" + 0.010*\"cubs\" + 0.009*\"his\" + 0.008*\"alomar\" + 0.008*\"dave\" + 0.007*\"hit\" + 0.007*\"appears\" + 0.007*\"talent\" + 0.006*\"hitter\" + 0.006*\"year\"')\n",
      "============================================================\n",
      "(14, '0.013*\"medical\" + 0.013*\"health\" + 0.011*\"msg\" + 0.011*\"disease\" + 0.010*\"and\" + 0.010*\"food\" + 0.008*\"patients\" + 0.006*\"aids\" + 0.006*\"treatment\" + 0.006*\"for\"')\n",
      "============================================================\n",
      "(15, '0.082*\"the\" + 0.031*\"and\" + 0.021*\"that\" + 0.015*\"you\" + 0.012*\"for\" + 0.011*\"they\" + 0.011*\"have\" + 0.011*\"was\" + 0.010*\"with\" + 0.010*\"this\"')\n",
      "============================================================\n",
      "(16, '0.279*\"max\" + 0.025*\"bhj\" + 0.024*\"giz\" + 0.014*\"qax\" + 0.011*\"bxn\" + 0.009*\"min\" + 0.006*\"nrhj\" + 0.006*\"okz\" + 0.006*\"biz\" + 0.005*\"ott\"')\n",
      "============================================================\n",
      "(17, '0.013*\"deskjet\" + 0.012*\"ink\" + 0.010*\"printer\" + 0.009*\"manes\" + 0.008*\"magpie\" + 0.008*\"linknet\" + 0.007*\"sho\" + 0.006*\"toner\" + 0.006*\"laser\" + 0.006*\"seller\"')\n",
      "============================================================\n",
      "(18, '0.084*\"the\" + 0.027*\"and\" + 0.011*\"for\" + 0.008*\"from\" + 0.007*\"that\" + 0.007*\"their\" + 0.007*\"was\" + 0.006*\"were\" + 0.006*\"people\" + 0.006*\"government\"')\n",
      "============================================================\n",
      "(19, '0.015*\"phillies\" + 0.011*\"speedy\" + 0.009*\"corn\" + 0.008*\"engr\" + 0.007*\"sports\" + 0.006*\"latech\" + 0.005*\"wip\" + 0.005*\"udel\" + 0.005*\"reg\" + 0.004*\"registers\"')\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    print('='*60) ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic distribution by text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " text_num : 0 : [(11, 0.087477356), (15, 0.89907825)]\n",
      "============================================================\n",
      " text_num : 1 : [(0, 0.22481254), (4, 0.1085002), (11, 0.32920027), (15, 0.23399226), (18, 0.09359312)]\n",
      "============================================================\n",
      " text_num : 2 : [(0, 0.05326696), (3, 0.030249566), (4, 0.055214375), (11, 0.19569851), (15, 0.6621454)]\n",
      "============================================================\n",
      " text_num : 3 : [(0, 0.09049226), (1, 0.1026771), (11, 0.31038532), (15, 0.44106385), (17, 0.04170013)]\n",
      "============================================================\n",
      " text_num : 4 : [(1, 0.10365048), (4, 0.29804382), (6, 0.027303578), (10, 0.0122889625), (11, 0.27612337), (15, 0.23263854), (18, 0.044315405)]\n",
      "============================================================\n",
      " text_num : 5 : [(1, 0.1236303), (4, 0.3717101), (6, 0.04107518), (10, 0.026564334), (15, 0.14156517), (18, 0.29312345)]\n",
      "============================================================\n",
      " text_num : 6 : [(11, 0.31914705), (14, 0.016620371), (15, 0.55174357), (18, 0.094680406)]\n",
      "============================================================\n",
      " text_num : 7 : [(0, 0.5385661), (1, 0.01861356), (4, 0.19817573), (11, 0.09643086), (12, 0.09988304), (15, 0.025428852), (19, 0.020625718)]\n",
      "============================================================\n",
      " text_num : 8 : [(11, 0.82249403), (15, 0.14146975)]\n",
      "============================================================\n",
      " text_num : 9 : [(0, 0.23261598), (11, 0.30091503), (12, 0.025311721), (15, 0.43616608)]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if idx == 10:\n",
    "        break\n",
    "        \n",
    "    print(f\" text_num : {idx} : {topic_list}\")\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n번째  text에서 토픽이 차지하는 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topictable(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "    \n",
    "    for idx, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list\n",
    "        doc = sorted(doc, key = lambda x: x[1], reverse = True) ## 내림차순 정렬\n",
    "\n",
    "        for j,(topic_num, score) in enumerate(doc):\n",
    "            if j == 0:\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(score,4), topic_list]), ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return topic_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topictable(ldamodel, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['문서 번호', '가장 비중높은 토픽', '비중 높은 토픽의 비중','각 토픽의 비중']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중높은 토픽</th>\n",
       "      <th>비중 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>[(11, 0.08743862), (15, 0.89911693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3292</td>\n",
       "      <td>[(0, 0.22483087), (4, 0.108510405), (11, 0.329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>[(0, 0.053271353), (3, 0.030250825), (4, 0.055...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.4410</td>\n",
       "      <td>[(0, 0.09042129), (1, 0.10267393), (11, 0.3105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>[(1, 0.10364907), (4, 0.29824147), (6, 0.02730...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>11309</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5499</td>\n",
       "      <td>[(4, 0.28352305), (10, 0.023721097), (14, 0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>11310</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>[(0, 0.35070455), (14, 0.028084742), (15, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>11311</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.7545</td>\n",
       "      <td>[(0, 0.23475595), (15, 0.75451803)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>11312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4717</td>\n",
       "      <td>[(1, 0.032270037), (4, 0.47173855), (10, 0.030...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>11313</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.7271</td>\n",
       "      <td>[(10, 0.19066086), (11, 0.058611155), (15, 0.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       문서 번호  가장 비중높은 토픽  비중 높은 토픽의 비중  \\\n",
       "0          0        15.0        0.8991   \n",
       "1          1        11.0        0.3292   \n",
       "2          2        15.0        0.6621   \n",
       "3          3        15.0        0.4410   \n",
       "4          4         4.0        0.2982   \n",
       "...      ...         ...           ...   \n",
       "11309  11309        15.0        0.5499   \n",
       "11310  11310        15.0        0.6115   \n",
       "11311  11311        15.0        0.7545   \n",
       "11312  11312         4.0        0.4717   \n",
       "11313  11313        15.0        0.7271   \n",
       "\n",
       "                                                각 토픽의 비중  \n",
       "0                   [(11, 0.08743862), (15, 0.89911693)]  \n",
       "1      [(0, 0.22483087), (4, 0.108510405), (11, 0.329...  \n",
       "2      [(0, 0.053271353), (3, 0.030250825), (4, 0.055...  \n",
       "3      [(0, 0.09042129), (1, 0.10267393), (11, 0.3105...  \n",
       "4      [(1, 0.10364907), (4, 0.29824147), (6, 0.02730...  \n",
       "...                                                  ...  \n",
       "11309  [(4, 0.28352305), (10, 0.023721097), (14, 0.13...  \n",
       "11310  [(0, 0.35070455), (14, 0.028084742), (15, 0.61...  \n",
       "11311                [(0, 0.23475595), (15, 0.75451803)]  \n",
       "11312  [(1, 0.032270037), (4, 0.47173855), (10, 0.030...  \n",
       "11313  [(10, 0.19066086), (11, 0.058611155), (15, 0.7...  \n",
       "\n",
       "[11314 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
