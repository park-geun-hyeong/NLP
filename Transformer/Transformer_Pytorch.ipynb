{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "- Transformer은 input sentence를 넣어서 output sentence를 출력해 내는 구조이다\n",
    "- 크게 Encoder, Decoder 구조로 이루어져 있다\n",
    "- Sequential Data를 다루며 병렬 처리를 할 수 있다는 점이 가장 큰 contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,x,s):\n",
    "        encoder_out = self.encoder(x)\n",
    "        decoder_out = self.decoder(s, encoder_out)\n",
    "        \n",
    "        return decoder_out\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- Encoder는 Encoder layer가 n개 쌓여있는 구조(논문에서는 n=6)\n",
    "- Encoder layer의 input과 output의 shape은 동일하다\n",
    "- Encoder layer는 input으로 들어오는 vector에 대해서 더 높은 차원에서의 context를 담는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_layer, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = []\n",
    "        for i in range(n_layers):\n",
    "            self.encoder.append(copy.deepcopy(self.encoder_layer))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.encoder:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer\n",
    "- Encoder layer는 크기 multi-head-attention-layer와 Position-wise-feed-forward-layer로 이루어져 있다.\n",
    "- 중간중간엔 add&norm layer가 끼어져 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention_layer = multi_head_attention_layer\n",
    "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.multi_head_attention_layer(x)\n",
    "        out = self.position_wise_feed_forward_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "- Attention이라는 것은 넓은 범위의 전체 data에 대해서 특정한 부분에 집중한다는 의미이다\n",
    "- Self-Attention은 문장에 token이 n개 있다고 가정할 경우, n×n 번 연산을 수행해 모든 token들 사이의 관계를 직접 구해낸다. 중간의 다른 token들을 거치지 않고 바로 direct한 관계를 구하는 것이기 때문에 Recurrent Network에 비해 더 명확하게 관계를 잡아낼 수 있다.\n",
    "- Query, Key, Value의 3가지 vector를 사용한다.\n",
    "- Query는 현재 시점의 token\n",
    "- Key, Value는 구하고자 하는 대상의 token(의미적으로는 같지만 다른 fc layer를 거치기 때문에 다른 값을 가지고 있다)\n",
    "- Query Attention(Q,K,V) = softmax(QK^T/sqrt(D_k))V\n",
    "- (batch, seq_len, d_emb)의 shape을 가진 text 토큰들이있다. 이들은 fc layer를 거쳐 (batch, seq_len, d_k)의 shape을 가진 Q,K,V vector로 변환한다, 이후 Q.matmul(k.T)를 거쳐 (batch,seq_len,seq_len)구조의 attention score를 만들어내고, mask_padding과 softmax, norm과정을 거친 후 V와 matmul을 진행해준다. 최종적으로는 input shape과 같은 (batch, seq_len, d_emb)형태의 vector가 출력된다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(self, query, key, value, mask):\n",
    "    d_k = key.size(-1)\n",
    "    attention_score = query.matmul(key.transpose(-2,-1))\n",
    "    attention_score = attention_score / math.sqrt(d_k) ## scaling\n",
    "    \n",
    "    if mask is not None: ## masking(masking은 대개 scaling과 softmax사이에 이루어진다)\n",
    "        attention_score = score.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "    attention_prob = F.softmax(score, dim = -1)\n",
    "    \n",
    "    output = attention_prob.matmul(value)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "- encoder layer에서 h개의 self attention을 진행해 그 결과를 종합하여 사용하는 것\n",
    "- d_model = h*d_k  \n",
    "- (seq_len, d_k)의 q,k,v를 h번 연산하는것이 아닌 fc layer를 통해 (seq_len, d_model)의 q,k,v를 생성하여 한번에 계산한다\n",
    "- Multi-Head Attention Layer의 개념적인 의미는 사실 단지 dk의 크기를 dmodel로 확장시키는 단순한 구현으로 끝난다는 점이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, qkv_fc_layer, fc_layer):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.query_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.key_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.value_vc_layer = copy.deepcopy(qkv_vc_layer)\n",
    "        self.fc_layer = fc_layer\n",
    "        \n",
    "        \n",
    "    def forward(self, query, key, value, mask=None): ## Q,K,V는 실제 q,k.v vector가 아닌 sentence matrix이다\n",
    "        \n",
    "        n_batch = query.shape[0]\n",
    "        \n",
    "        def transform(x, fc_layer): ## (batch, len_seq, n_emb) ==> (batch, h, len_seq, d_k)\n",
    "            out = fc_layer(x) ## (batch, len_seq, n_model)\n",
    "            out = out.view(n_batch, -1, self.h, self.d_model/self.h) ## (batch, len_seq, h, d_k)\n",
    "            out = out.transpose(1,2) ## (batch, h, len_seq, d_k)  ==> self attention진행시 d_k를 기준으로 q.k의 행렬곱이 이루어지므로 마지막 차원은 d_k이여야 한다\n",
    "                                                                # ==> 또한 mask matrix의 shape이 (batch, seq_len, seq_len)이므로 -2번째 shape은 seq_len과 같아야 한다           \n",
    "            return out\n",
    "        \n",
    "        query = transform(query, self.query_fc_layer)\n",
    "        key = transform(key, self.key_fc_layer)\n",
    "        value = transform(value, self.value_fc_layer)\n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) ## (batch, 1 , len_seq, len_seq)\n",
    "            \n",
    "        out = self.cacluate_attention(query, key, value, mask) ## (batch, h, len_seq, d_k)\n",
    "        out = out.transpose(1,2) ## (batch, len_seq, h, d_k)\n",
    "        out = contiguous().view(n_batch, -1, self.d_model) ##(batch, len_seq, d_model)\n",
    "        out = self.fc_layer(out) ## (batch, len_seq, d_emb)\n",
    "        \n",
    "        return out\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention_layer = multi_head_attention_layer\n",
    "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        out = self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask)\n",
    "        out = self.position_wise_feed_forward_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_layer, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = []\n",
    "        for i in range(n_layers):\n",
    "            self.encoder.append(copy.deepcopy(self.encoder_layer))\n",
    "            \n",
    "    def forward(self, x, maks):\n",
    "        out = x\n",
    "        for layer in self.encoder:\n",
    "            out = layer(out,mask)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,src, trg, mask):\n",
    "        encoder_out = self.encoder(src,mask)\n",
    "        decoder_out = self.decoder(trg, encoder_out)\n",
    "        \n",
    "        return decoder_out\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position wise feed forward layer\n",
    "- 단순히 2개의 fc layer를 가진다 (d_emb, d_ff) (d_ff, d_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, first_layer, second_layer):\n",
    "        super(PositionWiseFeedForwardLayer, self).__init__()\n",
    "        \n",
    "        self.first_layer = first_layer\n",
    "        self.second_layer = second_layer\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.first_layer(x)\n",
    "        out = F.relu(out)\n",
    "        out = dropout(out)\n",
    "        out = self.second_layer(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm layer\n",
    "- MultiHeadAttentionLayer와 PositionWiseFeedForwardLayer은 residual connection로 둘러쌓여져 있다\n",
    "- residual connection이란 y = f(x)+x를 의미한다\n",
    "- 즉 output만을 사용하는 것이 아닌 output+input을 사용한다 이를 통해서 back propagation 진행시 발생할 수 있는 gradient vanishing문제를 해결할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnectionLayer(nn.Module):\n",
    "    def __init__(self, norm_layer):\n",
    "        super(REsidualConnectionLayer, self).__init__()\n",
    "        \n",
    "        self.norm_layer = norm_layer\n",
    "        \n",
    "        \n",
    "    def forward(self,x, sub_layer):\n",
    "        out = sub_layer(x) + x\n",
    "        out = self.norm_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Encoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer, norm_layer):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention_layer = multi_head_attention_layer\n",
    "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
    "        self.rcl = [self.ResidualConnectionLayer(copy.deepcopy(norm_layer)) for i in range(2)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        out = self.rcl[0](x, lambda x : self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask))\n",
    "        out = self.rcl[1](x, lambda x : self.position_wise_feed_forward_layer(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- context vector와 right shifted sentence를 input으로 받아서 sentence를 output으로 생성한다(Teacher forcing)\n",
    "- right shifted sentence ==> ground truth[:-1]을 뜻한다\n",
    "- Teacher forcing in transformer ==> subsequent masking\n",
    "- 병렬 연산을 위해 ground truth의 embedding을 matrix로 만들어 input으로 그대로 사용하게 되면, Decoder에서 Self-Attention 연산을 수행하게 될 때 현재 출력해내야 하는 token의 정답까지 알고 있는 상황이 발생한다. 따라서 masking을 적용해야 한다. i번째 token을 생성해낼 때, 1∼i−1의 token은 보이지 않도록 처리를 해야 하는 것이다. 이러한 masking 기법을 subsequent masking이라고 한다.\n",
    "- 마찬가지로 n개의 decoder layer로 구성되어 있고 각각의 decoder layer는 MaskedMultiHeadAttentionLayer - MultiHeadAttentionLayer - FeedForwardLayer로 이루어져 있다   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    atten_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(atten_shape).astype('uint8'), k=1)\n",
    "    return torch.from_numpy(mask) == 0\n",
    "\n",
    "def make_std_mask(tgt, pad):\n",
    "    tgt_mask = (tgt!=pad)\n",
    "    tgt_mask = tgt_mask.unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask&Variable(subsequent_mask(tgt.size(-1)).type_as(tgt.data))\n",
    "    \n",
    "    return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,src, trg, src_mask, trg_mask):\n",
    "        encoder_out = self.encoder(src,src_mask)\n",
    "        decoder_out = self.decoder(trg, trg_mask, encoder_out)\n",
    "        \n",
    "        return decoder_out\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked multi head attention layer\n",
    "- mask로 들어오는 인자가 pad masking + subsequent masking이라는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mult head attention layer\n",
    "- masked multi head attention layer의 output + encoder의 context vector를 input으로 받는다\n",
    "- teaching forcing을 통해 건너온 masked multihead attenition layer의 output을 Query로 encoder의 context vector를 key, value로 삼는다 \n",
    "- 즉 Decodr layer에서의 multi head attention layer는 label data와 input data와의 attention을 구하는 구간이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder_layer, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder=[]\n",
    "        for i in range(n_layers):\n",
    "            self.decoder.append(copy.deepcopy(decoder_layer))\n",
    "            \n",
    "    def forward(self, mask, encoder_output, encoder_mask):\n",
    "        out = x \n",
    "        for layer in decoder:\n",
    "            out = layer(out, mask, encoder_output, encoder_mask)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, masked_multi_head_attention_layer, multi_head_attention_layer, position_wise_feed_forward_layer, norm_layer):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.masked_multi_head_attention_layer = ResidualConnectionLayer(masked_multi_head_attention_layer, copy.deepcopy(norm_layer))\n",
    "        self.multi_head_attention_layer = ResidualConnectionLayer(multi_head_attention_layer, copy.deepcopy(norm_layer))\n",
    "        self.position_wise_feed_forward_layer = ResidualConnectionLayer(position_wise_feed_forward_layer, copy.deepcopy(norm_layer))\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask, encoder_output, encoder_mask):\n",
    "        out = self.masked_multi_head_attention_layer(query=x, key=x, value=x, mask=mask)\n",
    "        out = self.multi_head_attention_layer(query=out, key= encoder_output, value=encoder_output, mask= encoder_mask)\n",
    "        out = self.position_wise_feed_forward_layer(x=out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,src, trg, src_mask, trg_mask):\n",
    "        encoder_out = self.encoder(src,src_mask)\n",
    "        decoder_out = self.decoder(trg, trg_mask, encoder_out, src_mask)\n",
    "        \n",
    "        return decoder_out\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer input\n",
    "- Embedding + Positional_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, embedding, positional_encoding):\n",
    "        super(TransformerEncoding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Sequential(embedding, positional_encoding) \n",
    "        \n",
    "    def forward(self ,x):\n",
    "        embedding = self.embedding(x)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "- 단순 embedding + scaling(sqrt(n_emb)를 곱해주기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embedding, vocab, n_emb):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(vocab), n_emb)\n",
    "        self.vocab = vocab\n",
    "        self.n_emb = n_emb\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x) * math.sqrt(slef.n_emb) ## scaling\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Positional Encoding \n",
    "-  PositionalEncoding의 목적은 positional정보(대표적으로 token의 순서, 즉 index number)를 정규화시키기 위한 것이다\n",
    "- sin 함수와 cos함수를 사용하는데, 짝수 index에는 sin함수를, 홀수 index에는 cos함수를 사용하게 된다. 이를 사용할 경우 항상 -1에서 1 사이의 값만이 positional 정보로 사용되게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_embed, max_seq_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(max_seq_len, d_embed)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + Variable(self.encoding[:, :x.size(1)], requires_grad=False) ## positional encoding은 학습되는 parameter가 아니다!\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_emb, trg_emb, encoder, decoder_out):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.src_emb = src_emb\n",
    "        self.trg_emb = trg_emb\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,src, trg, src_mask, trg_mask):\n",
    "        encoder_out = self.encoder(self.src_emb(src),src_mask)\n",
    "        decoder_out = self.decoder(self.trc_emb(trg), trg_mask, encoder_out, src_mask)\n",
    "        \n",
    "        return decoder_out\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Decoder (generater)\n",
    "- Decoder의 output이 그대로 Transformer의 최종 output이 되는 것은 아니다. 추가적인 layer를 거쳐간다. 이 layer들을 generator라고 부른다.\n",
    "- 우리가 결국 해내고자 하는 목표는 Decoder의 output이 sentence, 즉 token의 sequence가 되는 것이다. 그런데 Decoder의 output은 그저 (n_batch×seq_len×d_model)의 shape를 갖는 matrix일 뿐이다. 이를 vocabulary를 사용해 실제 token으로 변환할 수 있도록 차원을 수정해야 한다. 따라서 FC Layer를 거쳐 마지막 dimension을 d_model에서 len(vocab)으로 변경한다. 그래야 실제 vocabulary 내 token에 대응시킬 수 있는 값이 되기 때문이다. 이후 softmax 함수를 사용해 각 vocabulary에 대한 확률값으로 변환하게 되는데, 이 때 log_softmax를 사용해 성능을 향상시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_emb, trg_emb, encoder, decoder, fc_layer):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.src_emb = src_emb\n",
    "        self.trg_emb = trg_emb\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc_layer = fc_layer \n",
    "        \n",
    "    def forward(self,src, trg, src_mask, trg_mask):\n",
    "        encoder_out = self.encoder(self.src_emb(src),src_mask)\n",
    "        decoder_out = self.decoder(self.trc_emb(trg), trg_mask, encoder_out, src_mask)\n",
    "        out = self.fc_layer(decoder_out)\n",
    "        out = F.log_softmax(out, dim=-1)    \n",
    "        \n",
    "        return out\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, trg_vocab, n_emb=512, n_layer=6, d_model=512, h=8, d_ff=2048):\n",
    "    \n",
    "    cp = lambda x : copy.deepcopy(x)\n",
    "    \n",
    "    multi_head_attention_layer = MultiHeadAttentionLayer(d_model= d_model, \n",
    "                                                         h=h, \n",
    "                                                         qkv_fc_layer = nn.Linear(n_emb, d_model),\n",
    "                                                         fc_layer = nn.Linear(d_model, n_emb)) \n",
    "    \n",
    "    position_wise_feed_forward_layer = PositionWiseFeedForwardLayer(first_layer=nn.Linear(n_emb, d_ff),\n",
    "                                                                   second_layer=nn.Linear(d_ff, n_emb))\n",
    "                                            \n",
    "        \n",
    "    norm_layer = nn.LayerNorm(n_emb, eps=1e-6)\n",
    "    \n",
    "\n",
    "    model = Transformer(\n",
    "        src_emb = TransformerEmbedding(embedding = Embedding(n_emb=n_emb,vocab=src_vocab),\n",
    "                                       positional_encoding = PositionalEncoding(n_emb=n_emb)),\n",
    "        \n",
    "        trg_emb = TransformerEmbedding(embedding= Embedding(n_emb=n_emb, vocab= trg_vocab),\n",
    "                                       positional_encoding=  PositionalEncoding(n_emb=n_emb)),\n",
    "        \n",
    "        encoder = Encoder(encoder_layer = EncoderLayer(multi_head_attention_layer=cp(multi_head_attention_layer), \n",
    "                                                       position_wise_feed_forward_layer=cp(position_wise_feed_forward_layer), \n",
    "                                                       norm_layer = cp(norm_layer)),n_layers=6),\n",
    "        \n",
    "        decoder = Decoder(decoder_layer= DecoderLayer(masked_multi_head_attention_layer = cp(masked_multi_head_attention_layer),\n",
    "                                                   multi_head_attention_layer=cp(multi_head_attention_layer),\n",
    "                                                   position_wise_feed_forward_layer=cp(position_wise_feed_forward_layer), \n",
    "                                                   norm_layer=cp(norm_layer)), n_layers = 6),\n",
    "        \n",
    "        fc_layer = nn.Linear(d_model, len(trg_vocab)))\n",
    "                                      \n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
